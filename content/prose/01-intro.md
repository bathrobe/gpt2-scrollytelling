## Introduction

I recently finished Andrej Karpathy's 1-2 punch of GPT from scratch, developing both his [pico-sized GPT](https://www.youtube.com/watch?v=kCc8FmEb1nY) with character-level tokens and Tiny Shakespeare, as well as the [heavier duty reproduction of GPT-2 124M](https://www.youtube.com/watch?v=l8pRSuU81PU&t=13511s), on 10B tokens of educational data and with a variety of production grade optimizations.

Both were a treat to study and a gift to world knowledge. The first one was mostly fun and gentle, while the second one (the subject of this post) felt like a genuine true rite of passage.

I've spent the past couple days banging my head on the desk while my training runs crash and overfit all while my expensive GPU servers burn the dollars away. Was it worth it? Learning is always worth it.
